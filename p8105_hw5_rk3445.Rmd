---
title: "p8105_hw5_rk3445"
author: "Rosie Kwon"
date: "2025-11-01"
output: html_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(rvest)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

A function that checks whether there are duplicate birthdays in the group
```{r}
duplicate_birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n
  
  repeated_bday
}
```

run this function 10000 times for each group size between 2 and 50. 
```{r}
#For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs.
sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, duplicate_birthday)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

A plot showing the probability as a function of group size
```{r}
sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point(size = 1) + 
  geom_line(color = "darkorange", size = 1, alpha = 0.5) + 
  labs(
    title = "Probability that at least two people share a birthday",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold")
  )
```

The plot shows the probability of at least two people sharing a birthday with group sizes ranging from 2 to 50. We observe that the probability is close to zero at small group sizes but increases rapidly between group sizes of 10 and 30. Beyond that, the slope of the curve begins to slow and after group size of 45 the probability approaches 1.0 , meaning that almost every group of more than 40 people will have at least two individuals sharing a birthday.



## Problem 2

Define simulation function

```{r}
sim_t_test = function(samp_size = 30, mu = 0, sigma = 5, alpha = 0.05){
  
  results = 
    map(1:5000, ~ rnorm(n = samp_size, mean = mu, sd = sigma)) |> 
    map(~ t.test(.x, mu = 0)) |> 
    map_df(~ broom::tidy(.x)) |> 
    select(estimate, p.value) |> 
    mutate(reject_null = p.value < alpha)
  
  return(results)
}
```


Run simulation for true means from 0 to 6

```{r}
t_test_results =
  expand_grid(mu = 0:6) |> 
  mutate(result = map(mu, ~ sim_t_test(mu = .x))) |> 
  unnest()
```

**Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of ðœ‡ on the x axis. Describe the association between effect size and power.**

```{r}
power_results = 
t_test_results |> 
  group_by(mu) |> 
  summarize(
    proportion = mean(reject_null)
  )
```

```{r}
power_results |> 
  ggplot(aes(x = mu, y = proportion)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(size = 2, color = "darkblue") +
  scale_x_continuous(breaks = 0:6) +
  labs(
    title = "Power Curve of One-Sample t-test",
    x = expression("True mean ("*mu*")"),
    y = "Proportion rejected (Power)"
  ) +
  theme_minimal(base_size = 12)
```

When Î¼ = 0, the null hypothesis is true so that the test rejects the null about 5% of the time, matching with Î± = 0.05 significance level. As Î¼ increases, the effect size grows, the test becomes more sensitive and more likely to reject the null hypothesis. Therefore, the power (probability of rejecting null hypothesis) increases approaching 1 as effect size increases.



**Make a plot showing the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡on the x axis. Make a second plot (or overlay on the first) the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected on the y axis and the true value of ðœ‡ on the x axis.**

Compute the averages

```{r}
avg_estimates =
  t_test_results |>
  group_by(mu) |>
  summarize(
    mean_est_all = mean(estimate),
    mean_est_reject = mean(estimate[reject_null]),
    .groups = "drop"
  )
```
 

```{r}
avg_estimates |>
  ggplot(aes(x = mu)) +
  geom_line(aes(y = mean_est_all, color = "All samples")) +
  geom_point(aes(y = mean_est_all, color = "All samples"), size = 1.5) +
  geom_line(aes(y = mean_est_reject, color = "Null rejected"), linetype = "dashed") +
  geom_point(aes(y = mean_est_reject, color = "Null rejected"), size = 1.5) +
  scale_color_manual(values = c("All samples" = "blue", "Null rejected" = "red")) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  scale_x_continuous(breaks = 0:6) +
  labs(
    title = "Average Estimate of Î¼Ì‚ by True Mean (Î¼)",
    x = expression("True mean ("*mu*")"),
    y = expression("Average estimate ("*hat(mu)*")"),
    color = "Condition"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")

```

**Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡? Why or why not? **

The plot compares the average estimate of ðœ‡Ì‚ across all samples with the average estimate computed only from samples where the null hypothesis was rejected. The sample average estimates for all samples are close to the true mean ðœ‡, showing that the estimator is unbiased. However, the sample average of ðœ‡Ì‚ for which the null is rejected is higher than the true mean especially for small effect sizes. From when ðœ‡ is 5, the estimate value is approximately equal to the true mean. This is because we are conditioning on significance, so only samples with unusually large sample means (far from 0) tend to appear among the rejected cases.


# Problem 3

Load the dataset
```{r, message= FALSE}
homicides = 
  read_csv("data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(
    city_state = paste(city , state, sep = ", "),
    solve_status = case_match(
      disposition,
      "Closed without arrest" ~ 1,
      "Open/No arrest" ~ 1,
      "Closed by arrest" ~ 0
    )
  ) 
```

The dataset from Washington Post homicide data contains information about murders in U.S. cities, including:

- `uid`: unique identifier for each case

- `victim_last`, `victim_first`: victimâ€™s name

- `victim_age`, `victim_sex`, `victim_race`: demographics of victims

- `city`, `state`: location

- `disposition`: whether the case was solved or not


```{r}
#Create a city_state variable (e.g. â€œBaltimore, MDâ€) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).
homicides |> 
  group_by(city) |> 
  summarize(
    num_homicides = n(),
    unsolved = sum(solve_status, na.rm = TRUE) 
  ) |> 
  arrange(desc(num_homicides)) |> 
  knitr::kable()
```



